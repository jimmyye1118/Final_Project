import sounddevice as sd
import numpy as np
import tempfile
import wave
import pyttsx3
from sentence_transformers import SentenceTransformer, util
from faster_whisper import WhisperModel
import threading

class VoiceRecognizer:
    def __init__(self):
        """åˆå§‹åŒ–èªéŸ³è¾¨è­˜èˆ‡èªéŸ³åˆæˆæ¨¡å‹"""
        print("æ­£åœ¨è¼‰å…¥èªéŸ³è¾¨è­˜èˆ‡èªæ„æ¨¡å‹...")
        # åˆå§‹åŒ–èªéŸ³è¾¨è­˜æ¨¡å‹ï¼ˆä½¿ç”¨ CPUï¼‰
        self.whisper_model = WhisperModel("base", device="cpu", compute_type="int8")
        # åˆå§‹åŒ–èªæ„æ¯”è¼ƒèˆ‡èªéŸ³åˆæˆæ¨¡å‹
        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.tts_engine = pyttsx3.init()
        
        # é—œéµå­—èˆ‡å‹•ä½œ
        self.keyword_actions = {
            "é–‹å§‹": lambda: "start",
            "æš«åœ": lambda: "stop",
            "ç´…è‰²": lambda: "red",
            "è—è‰²": lambda: "blue",
            "é»ƒè‰²": lambda: "yellow",
            "ç¶ è‰²": lambda: "green",
        }
        
        # é å…ˆå»ºç«‹èªæ„å‘é‡
        self.keyword_list = list(self.keyword_actions.keys())
        self.keyword_embeddings = self.embedding_model.encode(self.keyword_list, convert_to_tensor=True)
        
        self.recording_data = []
        self.is_recording = False
        self.fs = 16000  # å–æ¨£ç‡
        self.lock = threading.Lock()
        print("èªéŸ³æ¨¡çµ„è¼‰å…¥å®Œæˆã€‚")

    def _audio_callback(self, indata, frames, time, status):
        """éŒ„éŸ³çš„å›èª¿å‡½æ•¸"""
        if self.is_recording:
            self.recording_data.append(indata.copy())

    def start_recording(self):
        """å•Ÿå‹•éŒ„éŸ³"""
        with self.lock:
            if self.is_recording:
                print("éŒ„éŸ³å·²åœ¨é€²è¡Œä¸­ã€‚")
                return
            self.is_recording = True
            self.recording_data = []
            print("ğŸ™ï¸ é–‹å§‹éŒ„éŸ³...")
            self.stream = sd.InputStream(
                samplerate=self.fs, 
                channels=1, 
                dtype='int16', 
                callback=self._audio_callback
            )
            self.stream.start()

    def stop_recording(self):
        """åœæ­¢éŒ„éŸ³ä¸¦å›å‚³è¾¨è­˜çµæœèˆ‡èªéŸ³å…§å®¹"""
        with self.lock:
            if not self.is_recording:
                print("éŒ„éŸ³æœªåœ¨é€²è¡Œä¸­ã€‚")
                return None, None

            self.is_recording = False
            self.stream.stop()
            self.stream.close()

        print("ğŸ›‘ éŒ„éŸ³çµæŸï¼Œæ­£åœ¨è¾¨è­˜...")

        audio_file_path = self._save_audio_to_file()
        if audio_file_path:
            command, text = self._process_command(audio_file_path)
            return command, text
        return None, None


    def _save_audio_to_file(self):
        """å°‡éŒ„éŸ³è³‡æ–™å„²å­˜ç‚ºæš«å­˜ WAV æª”æ¡ˆ"""
        if not self.recording_data:
            print("æ²’æœ‰éŒ„éŸ³è³‡æ–™ã€‚")
            return None
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            with wave.open(f.name, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.fs)
                audio_data = np.concatenate(self.recording_data, axis=0)
                wf.writeframes(audio_data.tobytes())
            return f.name

    def _process_command(self, audio_path):
        """è™•ç†èªéŸ³æŒ‡ä»¤ï¼Œé€²è¡Œè¾¨è­˜èˆ‡èªæ„æ¯”å°ï¼Œå›å‚³ (command, text)"""
        segments, _ = self.whisper_model.transcribe(audio_path, beam_size=5, language="zh")
        text = ''.join([segment.text for segment in segments]).strip()
        print(f"ğŸ“ è¾¨è­˜çµæœï¼š{text}")
        
        if not text:
            self.speak("æ²’æœ‰è½æ¸…æ¥šï¼Œè«‹å†èªªä¸€æ¬¡ã€‚")
            return None, text  # å³ return (None, "")
            
        user_embedding = self.embedding_model.encode(text, convert_to_tensor=True)
        cosine_scores = util.cos_sim(user_embedding, self.keyword_embeddings)
        best_idx = cosine_scores.argmax()
        best_score = cosine_scores[0][best_idx].item()
        best_keyword = self.keyword_list[best_idx]
        
        print(f"ğŸ” æœ€ç›¸ä¼¼ï¼š{best_keyword}ï¼ˆåˆ†æ•¸ï¼š{best_score:.2f}ï¼‰")
        
        if best_score >= 0.6:
            return self.keyword_actions[best_keyword](), text  # å›å‚³æŒ‡ä»¤èˆ‡è¾¨è­˜çµæœæ–‡å­—
        else:
            self.speak("ç„¡æ³•ç†è§£æ‚¨çš„æŒ‡ä»¤ï¼Œè«‹å†èªªä¸€æ¬¡ã€‚")
            return None, text


    def speak(self, text):
        """èªéŸ³åˆæˆåŠŸèƒ½"""
        print(f"TTS èªªè©±ï¼š{text}")
        self.tts_engine.say(text)
        self.tts_engine.runAndWait()

# æ¸¬è©¦ç”¨ï¼Œæ‚¨å¯ä»¥ç¨ç«‹åŸ·è¡Œæ­¤æª”æ¡ˆä¾†æ¸¬è©¦èªéŸ³æ¨¡çµ„
if __name__ == '__main__':
    vr = VoiceRecognizer()
    
    print("ğŸ“¢ æŒ‰ä¸‹ 'g' é–‹å§‹éŒ„éŸ³ï¼Œ's' åœæ­¢ä¸¦è¾¨è­˜ï¼Œ'q' é€€å‡º")
    while True:
        key = input("â¡ï¸ è¼¸å…¥æŒ‡ä»¤ï¼š")
        if key == "g":
            vr.start_recording()
        elif key == "s":
            command = vr.stop_recording()
            if command:
                print(f"åŸ·è¡ŒæŒ‡ä»¤ï¼š{command}")
        elif key == "q":
            print("ğŸ‘‹ é€€å‡ºæ¸¬è©¦")
            break
        else:
            print("âš ï¸ ç„¡æ•ˆæŒ‡ä»¤ï¼Œè«‹è¼¸å…¥ g, s æˆ– q")